{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "from lamah_dataset import LamaHDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def ensure_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_step(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        train_loss += loss.item() * batch.num_graphs / len(train_dataset)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def eval_step(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            loss = F.mse_loss(out, batch.y)\n",
    "            eval_loss += loss.item() * batch.num_graphs / len(eval_dataset)\n",
    "    return eval_loss\n",
    "\n",
    "\n",
    "def train(model, train_dataset, eval_dataset, hparams):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=hparams[\"training\"][\"batch_size\"], shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=hparams[\"training\"][\"batch_size\"], shuffle=False)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=hparams[\"training\"][\"learning_rate\"],\n",
    "                             weight_decay=hparams[\"training\"][\"weight_decay\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {\"train_loss\": [], \"eval_loss\": [], \"model_params\": [], \"optim_params\": []}\n",
    "\n",
    "    for epoch in range(hparams[\"training\"][\"num_epochs\"]):\n",
    "        print(f\"Epoch {epoch + 1}/{hparams['training']['num_epochs']}\")\n",
    "\n",
    "        train_loss = train_step(model, train_loader, optimizer, device)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        eval_loss = eval_step(model, eval_loader, device)\n",
    "        history[\"eval_loss\"].append(eval_loss)\n",
    "\n",
    "        history[\"model_params\"].append(copy.deepcopy(model.state_dict()))\n",
    "        history[\"optim_params\"].append(copy.deepcopy(optimizer.state_dict()))\n",
    "\n",
    "        print(f\"Loss (train/eval): {train_loss:.4f}/{eval_loss:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"history\": history,\n",
    "        \"hparams\": hparams\n",
    "    }, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S_\" + model.__class__.__name__))\n",
    "\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_edge_weights(weight_type):\n",
    "    if weight_type == \"binary\":\n",
    "        return torch.ones(data.num_edges)\n",
    "    if weight_type == \"dist_hdn\":\n",
    "        return data.edge_attr[:, 0]\n",
    "    elif weight_type == \"elev_diff\":\n",
    "        return data.edge_attr[:, 1]\n",
    "    elif weight_type == \"strm_slope\":\n",
    "        return data.edge_attr[:, 2]\n",
    "    elif weight_type == \"disabled\":\n",
    "        return torch.zeros(data.num_edges)\n",
    "    elif weight_type == \"learned\":\n",
    "        return nn.Parameter(torch.ones(data.num_edges))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid weight type given!\")\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, window_size_hrs=24, num_hidden=2, hidden_size=128, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.layers = nn.ModuleList([nn.Linear(window_size_hrs, hidden_size) if i == 0 else\n",
    "                                     nn.Linear(hidden_size, 1) if i == num_hidden else\n",
    "                                     nn.Linear(hidden_size, hidden_size)\n",
    "                                     for i in range(0, num_hidden + 1)])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "\n",
    "        x = F.relu(self.layers[0](x))\n",
    "        for layer in self.layers[1:-1]:\n",
    "            x = F.relu(layer(x)) + (x if self.residual else 0)\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, window_size_hrs=24, num_convs=2, hidden_size=128, residual=False, weight_type=\"binary\"):\n",
    "        super().__init__()\n",
    "        self.dense_in = nn.Linear(window_size_hrs, hidden_size)\n",
    "        self.convs = nn.ModuleList([GCNConv(hidden_size, hidden_size)\n",
    "                                    for _ in range(num_convs)])\n",
    "        self.dense_out = nn.Linear(hidden_size, 1)\n",
    "        self.residual = residual\n",
    "        self.edge_weights = get_edge_weights(weight_type)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_idx = data.x, data.edge_index\n",
    "        edge_w = self.edge_weights.repeat(data.num_graphs)\n",
    "\n",
    "        x = F.relu(self.dense_in(x))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_idx, edge_w)) \\\n",
    "                + (x if self.residual else 0)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, window_size_hrs=24, num_hidden=2, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([GATConv(window_size_hrs, hidden_size) if i == 0 else\n",
    "                                     GATConv(hidden_size, 1) if i == num_hidden else\n",
    "                                     GATConv(hidden_size, hidden_size)\n",
    "                                     for i in range(0, num_hidden + 1)])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ensure_reproducibility(123456789)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:20<00:00, 18.10it/s]\n"
     ]
    }
   ],
   "source": [
    "HPARAMS = {\n",
    "    \"data\": {\n",
    "        \"window_size\": 24,\n",
    "        \"stride_length\": 1,\n",
    "        \"lead_time\": 1,\n",
    "        \"bidirectional\": False,\n",
    "        \"normalized\": True\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"propagation_dist\": 19,\n",
    "        \"hidden_size\": 128,\n",
    "        \"residual\": True,\n",
    "        \"weight_type\": \"binary\"\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_epochs\": 50,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"weight_decay\": 0  # 5e-4\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = LamaHDataset(\"LamaH-CE\",\n",
    "                       years=range(2000, 2018),\n",
    "                       window_size_hrs=HPARAMS[\"data\"][\"window_size\"],\n",
    "                       stride_length_hrs=HPARAMS[\"data\"][\"stride_length\"],\n",
    "                       lead_time_hrs=HPARAMS[\"data\"][\"lead_time\"],\n",
    "                       bidirectional=HPARAMS[\"data\"][\"bidirectional\"],\n",
    "                       normalize=HPARAMS[\"data\"][\"normalized\"])\n",
    "train_dataset = dataset.subsample_years(y for y in range(2000, 2018) if y % 8 > 0)\n",
    "eval_dataset = dataset.subsample_years(y for y in range(2000, 2018) if y % 8 == 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43meval_dataset\u001B[49m[\u001B[38;5;241m4200\u001B[39m]\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#vertices:\u001B[39m\u001B[38;5;124m\"\u001B[39m, data\u001B[38;5;241m.\u001B[39mnum_nodes)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#edges:\u001B[39m\u001B[38;5;124m\"\u001B[39m, data\u001B[38;5;241m.\u001B[39mnum_edges)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'eval_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data = eval_dataset[4200]\n",
    "print(\"# vertices:\", data.num_nodes)\n",
    "print(\"# edges:\", data.num_edges)\n",
    "print(\"# vertex features:\", data.num_node_features)\n",
    "print(\"# edge features:\", data.num_edge_features)\n",
    "print(\"# faces:\", data.num_faces)\n",
    "print(\"directed graph?:\", data.is_directed())\n",
    "print(\"isolated vertices?\", data.has_isolated_nodes())\n",
    "print(\"self-loops?\", data.has_self_loops())\n",
    "print(\"coalesced?\", data.is_coalesced())\n",
    "print(\"valid?\", data.validate())\n",
    "plt.figure(figsize=(12, 9))\n",
    "graph = to_networkx(data, to_undirected=False)\n",
    "nx.draw_networkx(graph, pos=nx.drawing.kamada_kawai_layout(graph), with_labels=True,\n",
    "                 node_color=[tuple(torch.rand(3)) for _ in range(375)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO try F.elu at some point\n",
    "#model = MLP(window_size_hrs=WINDOW_SIZE, num_hidden=19, hidden_size=128, residual=True).to(device)\n",
    "model = GCN(window_size_hrs=HPARAMS[\"data\"][\"window_size\"],\n",
    "            num_convs=HPARAMS[\"model\"][\"propagation_dist\"],\n",
    "            hidden_size=HPARAMS[\"model\"][\"hidden_size\"],\n",
    "            residual=HPARAMS[\"model\"][\"residual\"],\n",
    "            weight_type=HPARAMS[\"model\"][\"weight_type\"])\n",
    "print(summary(model, depth=2))\n",
    "\n",
    "history = train(model, train_dataset, eval_dataset, HPARAMS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"eval_loss\"], label=\"eval\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "i = history[\"eval_loss\"].index(min(history[\"eval_loss\"]))\n",
    "print(f\"{history['train_loss'][i]:.4f}/{history['eval_loss'][i]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
